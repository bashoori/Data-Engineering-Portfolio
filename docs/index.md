# 💼 Bita Ashoori | Data Engineering Portfolio

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/pp/IMG_9043.JPG" alt="Bita Ashoori Profile" width="140" style="border-radius: 50%;" />
</p>

## 👩‍💻 About Me

Welcome! I’m a Data Engineer based in Vancouver with over 5 years of experience across data engineering, business intelligence, and analytics. I’m passionate about clean architecture, automation, and helping organizations turn data into meaningful decisions. I’m currently expanding my knowledge in AI and machine learning to complement my strong background in data pipelines and cloud engineering.

I have over 3 years of hands-on experience building and maintaining cloud-based data pipelines, along with 2+ years as a BI/ETL Developer. I specialize in transforming raw data into actionable insights using Python, SQL, AWS (S3, Redshift, Lambda), and Apache Airflow.

I have a proven track record of designing ETL workflows that integrate data from APIs, JSON, and relational databases—delivering scalable, automated, and reliable data solutions. My background in BI enables me to bridge the gap between technical solutions and business requirements, translating complex needs into efficient data systems within agile environments.

I’m deeply committed to solving large-scale data problems through clean architecture, real-time insights, and automation—empowering organizations to extract true value from their data.

---

## 📌 Featured

📄 [Download My Resume](./bita_ashoori_resume.pdf)  
🚀 Currently exploring ML model deployment and real-time analytics use cases  
📬 Open to freelance and full-time remote opportunities

---

## 🚀 Projects

### 🛠️ Airflow AWS Modernization  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/airflow-aws-modernization)  
![Airflow](https://img.shields.io/badge/Airflow-017CEE?style=flat&logo=apache-airflow&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-232F3E?style=flat&logo=amazon-aws&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=flat&logo=docker&logoColor=white)

Migrated legacy Windows Task Scheduler jobs into modular Airflow DAGs with Docker and AWS S3. This project improves maintainability and scalability of previously manual and error-prone batch processes. It leverages Docker containers to simulate production workflows in a local dev environment.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/airflow-aws-modernization/etl2.png" alt="Airflow AWS Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### ⚡ Real-Time Marketing Pipeline  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/real-time-marketing-pipeline)  
![PySpark](https://img.shields.io/badge/PySpark-E34F26?style=flat&logo=apachespark&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-E0214E?style=flat&logo=databricks&logoColor=white)
![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-2088FF?style=flat&logo=github-actions&logoColor=white)

Built to simulate real-time ingestion of ad data, this PySpark pipeline automates campaign-level transformations and stores results in AWS S3. CI/CD is managed via GitHub Actions.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/real-time-marketing-pipeline/image1.png" alt="Real-Time Pipeline Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### ☁️ Cloud ETL Modernization  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/cloud-etl-modernization-airflow-aws)  
![Airflow](https://img.shields.io/badge/Airflow-017CEE?style=flat&logo=apache-airflow&logoColor=white)
![Redshift](https://img.shields.io/badge/AWS_Redshift-4053D6?style=flat&logo=amazon-redshift&logoColor=white)
![CloudWatch](https://img.shields.io/badge/AWS_CloudWatch-FF4F8B?style=flat&logo=amazon-aws&logoColor=white)

Rebuilt legacy ETL workflows on Airflow and AWS services to handle scalable processing and alerting. Uses Redshift as the destination with monitoring via CloudWatch.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/cloud-etl-Modernization/etl31.png" alt="Cloud ETL Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### 🏥 FHIR Healthcare Pipeline  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/healthcare-FHIR-data-pipeline)  
![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)
![FHIR](https://img.shields.io/badge/FHIR-DF3E51?style=flat&logo=fhir&logoColor=white)

Parsed synthetic FHIR-compliant healthcare data into structured relational tables using Python and Pandas. Data was visualized in Streamlit and prepared for BigQuery.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/healthcare-FHIR-data-pipeline/etl4.png" alt="FHIR Pipeline Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### 🔍 LinkedIn Scraper (Lambda)  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/linkedIn-job-scraper)  
![AWS Lambda](https://img.shields.io/badge/AWS_Lambda-FF9900?style=flat&logo=amazon-aws&logoColor=white)
![BeautifulSoup](https://img.shields.io/badge/BeautifulSoup-2C8EBB?style=flat&logo=python&logoColor=white)

Serverless job scraper that uses AWS Lambda, EventBridge, and BeautifulSoup to gather job data from LinkedIn and store it in S3. Alerting and logging are handled via SNS and CloudWatch.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/linkedIn-job-scraper/etl5.png" alt="LinkedIn Scraper Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### 📈 PySpark Sales Pipeline  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/pyspark-sales-pipeline)  
![PySpark](https://img.shields.io/badge/PySpark-E34F26?style=flat&logo=apachespark&logoColor=white)
![Delta Lake](https://img.shields.io/badge/Delta_Lake-0F9D58?style=flat&logo=databricks&logoColor=white)
![S3](https://img.shields.io/badge/S3-569A31?style=flat&logo=amazon-aws&logoColor=white)

A scalable pipeline to clean and transform raw CSV sales data into Delta Lake format. Results are optimized for BI use and stored in AWS S3.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/pyspark-sales-pipeline/etl6.png" alt="PySpark Pipeline Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

## 📫 Contact Me

📍 Vancouver, Canada  
🔗 [LinkedIn](https://linkedin.com/in/bashoori)  
💻 [GitHub](https://github.com/bashoori)  

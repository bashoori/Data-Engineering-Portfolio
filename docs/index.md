<table width="100%">
  <tr>
    <td>
      <h1>💼 Bita Ashoori | Data Engineering Portfolio</h1>
    </td>
    <td align="right">
      <img src="https://raw.githubusercontent.com/bashoori/repo/master/pp/IMG_9043.JPG" width="100" alt="Bita Ashoori" style="border-radius: 50%;" />
    </td>
  </tr>
</table>

## 👩‍💻 About Me

Welcome! I’m a Data Engineer based in Vancouver with over 5 years of experience across data engineering, business intelligence, and analytics. I’m passionate about clean architecture, automation, and helping organizations turn data into meaningful decisions. I’m currently expanding my knowledge in AI and machine learning to complement my strong background in data pipelines and cloud engineering.

I have over 3 years of hands-on experience building and maintaining cloud-based data pipelines, along with 2+ years as a BI/ETL Developer. I specialize in transforming raw data into actionable insights using Python, SQL, AWS (S3, Redshift, Lambda), and Apache Airflow.

---

## 📌 Featured

📄 [Download My Resume](./bita_ashoori_resume.pdf)  
🚀 Always eager to learn, grow, and sharpen my skills through hands-on projects  
📬 Open to freelance and full-time remote opportunities

---

## 📫 Contact Me

📍 Vancouver, Canada  
🔗 [LinkedIn](https://linkedin.com/in/bashoori)  
💻 [GitHub](https://github.com/bashoori)

---

## 🚀 Projects

### 🛠️ Airflow AWS Modernization  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/airflow-aws-modernization)  
📎 [GitHub Repo](https://github.com/bashoori/data-engineering-portfolio/tree/main/airflow-aws-modernization)  
🧰 **Stack**: Python, Apache Airflow, Docker, AWS S3  
🧪 **Tested On**: Local Docker, GitHub Codespaces  

Migrated legacy Windows Task Scheduler jobs into modular Airflow DAGs with Docker and AWS S3.  
This project improves maintainability and scalability of previously manual and error-prone batch processes. It leverages Docker containers to simulate production workflows in a local dev environment.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/airflow-aws-modernization/etl2.png" alt="Airflow AWS Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### ⚡ Real-Time Marketing Pipeline  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/real-time-marketing-pipeline)  
📎 [GitHub Repo](https://github.com/bashoori/data-engineering-portfolio/tree/main/real-time-marketing-pipeline)  
🧰 **Stack**: PySpark, Databricks, GitHub Actions, AWS S3  
🧪 **Tested On**: Databricks Community Edition, GitHub CI/CD  

Built to simulate real-time ingestion of ad data, this PySpark pipeline automates campaign-level transformations and stores results in AWS S3. CI/CD is managed via GitHub Actions.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/real-time-marketing-pipeline/image1.png" alt="Real-Time Pipeline Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### ☁️ Cloud ETL Modernization  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/cloud-etl-modernization-airflow-aws)  
📎 [GitHub Repo](https://github.com/bashoori/data-engineering-portfolio/tree/main/cloud-etl-modernization-airflow-aws)  
🧰 **Stack**: Apache Airflow, AWS Redshift, CloudWatch  
🧪 **Tested On**: AWS Free Tier, Docker  

Rebuilt legacy ETL workflows on Airflow and AWS services to handle scalable processing and alerting. Uses Redshift as the destination with monitoring via CloudWatch.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/cloud-etl-Modernization/etl31.png" alt="Cloud ETL Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### 🏥 FHIR Healthcare Pipeline  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/healthcare-FHIR-data-pipeline)  
📎 [GitHub Repo](https://github.com/bashoori/data-engineering-portfolio/tree/main/healthcare-FHIR-data-pipeline)  
🧰 **Stack**: Python, Pandas, Synthea, SQLite, Streamlit  
🧪 **Tested On**: Local, Streamlit, BigQuery-compatible output  

Parsed synthetic FHIR-compliant healthcare data into structured relational tables using Python and Pandas. Data was visualized in Streamlit and prepared for BigQuery.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/healthcare-FHIR-data-pipeline/etl4.png" alt="FHIR Pipeline Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### 🔍 LinkedIn Scraper (Lambda)  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/linkedIn-job-scraper)  
📎 [GitHub Repo](https://github.com/bashoori/data-engineering-portfolio/tree/main/linkedIn-job-scraper)  
🧰 **Stack**: AWS Lambda, EventBridge, BeautifulSoup, S3, SNS, CloudWatch  
🧪 **Tested On**: AWS Free Tier  

Serverless job scraper that uses AWS Lambda, EventBridge, and BeautifulSoup to gather job data from LinkedIn and store it in S3. Alerting and logging are handled via SNS and CloudWatch.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/linkedIn-job-scraper/etl5.png" alt="LinkedIn Scraper Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>

---

### 📈 PySpark Sales Pipeline  
[🔗 View Project](https://github.com/bashoori/data-engineering-portfolio/tree/main/pyspark-sales-pipeline)  
📎 [GitHub Repo](https://github.com/bashoori/data-engineering-portfolio/tree/main/pyspark-sales-pipeline)  
🧰 **Stack**: PySpark, Delta Lake, AWS S3  
🧪 **Tested On**: Local Databricks, AWS Storage  

A scalable pipeline to clean and transform raw CSV sales data into Delta Lake format. Results are optimized for BI use and stored in AWS S3.

<p align="center">
  <img src="https://raw.githubusercontent.com/bashoori/repo/master/pyspark-sales-pipeline/etl6.png" alt="PySpark Pipeline Diagram" width="700" style="border: 1px solid #ccc; border-radius: 6px;" />
</p>